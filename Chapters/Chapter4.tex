% Chapter 4

\chapter{Approach}

\label{approach}
The following sections will deeply explore and describe the insights of the project and the development process and the decisions taken with the reasons behind it. 
These sections imply a higher technical language and background filled with several domain-specific vocabularies.

%----------------------------------------------------------------------------------------

\section{Environment Architecture}
In this section, the different methodologies, programming tools and abstractions used during the project may be discussed and how the code is built around them.

\subsection{Python}

\begin{figure}[th]
    \centering
    \begin{center}
        \includegraphics[height=25mm]{Figures/python}
    \end{center}
    \decoRule
    \caption{Python logo}
    \label{fig:python logo}
\end{figure}


Python is a multi-purpose open-source programming language focused on the quick prototyping. Using a really simple language syntax makes any inexperienced programmer be able to comprehend and create simple scripts. 
Since there is no compilation, the debugging step is especially fast and repeating the cycle of editing and running is reduced in time\cite{pythonabout}.

Given the multiple open-source libraries and frameworks available for Machine Learning and Artificial Intelligence, Python is a great and suitable option to develop a project with the needed characteristics.
Some of the mentioned libraries that will be used along with the project:

\begin{itemize}
    \item NumPy: package for scientific computing with Python \cite{numpy}.
    \item pandas: library providing high-performance, easy-to-use data structures, and data analysis tools for the Python programming language \cite{pandas}.
    \item scikit-learn: Machine Learning module providing efficient tools for data mining and data analysis \cite{scikit}.
    \item Matplotlib  2D plotting library \cite{matplotlib}.
    \item seaborn: data visualization library. It provides a high-level interface for drawing attractive and informative statistical graphics \cite{seaborn}.
\end{itemize}


\subsection{MongoDB}

\begin{figure}[th]
    \centering
    \begin{center}
        \includegraphics[height=25mm]{Figures/mongodb}
    \end{center}
    \decoRule
    \caption{MongoDB logo}
    \label{fig:mongodb logo}
\end{figure}

MongoDB is a general-purpose, document-based, distributed database \cite{mongo}.
With a powerful query syntax and language is pretty straightforward to build complex databases and retrieve the desired records with ease.
Since it is based on the JSON\cite{json} standard it allows creating complex records using arrays, nested properties, and key-value pairs.

Despite the traditional relation Databases using the SQL standards, MongoDB is a nonrelational database relaying on the NoSQL (Not Only SQL) syntax and instead of using tables and rows as in relational databases, MongoDB is built around collections and documents.

Given the previously described problem, MongoDB is a perfect and suitable option to depend on for storing and retrieving the music datasets available. With just some simple commands is easy to obtain the desired data, a chunk of it or more complex subsets that need to fit special conditions to better approximate the model standards.

\newpage
\subsection{Tensorflow}

\begin{figure}[th]
    \centering
    \begin{center}
        \includegraphics[height=25mm]{Figures/tensorflow}
    \end{center}
    \decoRule
    \caption{Tensorflow logo}
    \label{fig:tensorflow logo}
\end{figure}

Tensorflow is an open-source mathematical computing library used as a backend for heavily expensive operations.
It offers different levels of abstractions so it can work easily coupled with different libraries that rely on heavy calculations and operations with matrixes or tensors.

It provides a collection of workflows to develop and train models using different programming languages so even though it was publicly released in 2015 it is the preferred option to build Machine Learning models as Artificial Neural Networks, the main topic of this thesis \cite{tensorflow}.

\subsection{Keras}

\begin{figure}[th]
    \centering
    \begin{center}
        \includegraphics[height=25mm]{Figures/keras}
    \end{center}
    \decoRule
    \caption{Keras logo}
    \label{fig:keras logo}
\end{figure}

Keras is a high-level neural networks API, capable of running on top of TensorFlow. 
It is focused on enabling fast experimentation and prototyping using a friendly user-interface API, modularity, and extensibility. 

Supports most of the most common known Artifical Neural Networks types such a Deep Neural Networks, Convolutional Neural Networks, and Recurrent Neural Networks or any combination of the previously mentioned ones. 
Nonetheless, implementing your custom layers and metrics is easy in case the basic framework does not suit your needs.

Relying on TensorFlow as the mathematical background allows running seamlessly on both the CPU and the GPU thus heavy models with extensive training phases can be easily computed across different machines alleviating the load from one system to multiple ones.

\newpage
%----------------------------------------------------------------------------------------

\section{Basic stack}
The following section briefly explains how the components already mentioned are connected and used during the whole project.
The project consists of several scripts that handle each one of the individual tasks, only the main ones will be discussed in this paper:

\begin{itemize}
    \item Database Warmer: This script takes the provided CSV files to process them and creates a ready-to-use document for each of the songs to be inserted into the MongoDB Database. This is a one-use-only script that is crucial to be able to retrieve the desired songs afterward.
    \begin{figure}[th]
        \centering
        \includegraphics{Figures/DatabaseWarmer}
        \decoRule
        \caption[Database Warmer Script]{Diagram detailing the database warming process}
        \label{fig:Database Warmer Script}
    \end{figure}


    Here is an example of one of the documents inserted:
    \begin{lstlisting}[language=json, caption=Song JSON document]
{
    "_id": "5cadfc2f4fac9e1e50b0ad5a",
    "mbid": "1a00a335-fead-46ec-8d4f-06e8341291ea",
    "release": "0f2ccf4d-d242-3c23-a419-ea548af51df3",
    "genres": [
        {
        "name": "electronic",
        "genres": [
            {
            "name": "ambient",
            "genres": []
            },
            {
            "name": "trance",
            "genres": []
            }
        ]
        }
    ]
}
    \end{lstlisting}

    
    \item Main Script This script is in charge of the entire process and stages model related:
    \begin{enumerate}
        
        \item Train: This is the base stage of the process, the script connects with the Database and retrieves the dataset to be used as training, loads and builds the model in Keras and proceeds to train the model using TensorFlow, once the training process is finished the trained model is stored in the disk and the results are evaluated in the test stage.
        \begin{figure}[th]
            \centering
            \includegraphics{Figures/TrainStage}
            \decoRule
            \caption[Train Stage]{Diagram detailing the training stage}
            \label{fig:Train Stage}
        \end{figure}
        
        \item Test: Once the model is trained its performance should be evaluated with data apart from the examples used in the training, to avoid the overfitting. For this case of the matter, the new examples are fed to the trained model and its outputs predicted and checked against its known outputs.
        \begin{figure}[th]
            \centering
            \includegraphics{Figures/TestStage}
            \decoRule
            \caption[Test Stage]{Diagram detailing the test stage}
            \label{fig:Test Stage}
        \end{figure}
        
        \item Predict: This phase is obtained once the proper model with the best outcome is built and trained. It properly labels new and unknown songs.
        \begin{figure}[th]
            \centering
            \includegraphics{Figures/PredictStage}
            \decoRule
            \caption[Predict stage]{Diagram detailing predict stage}
            \label{fig:Predict Stage}
        \end{figure}

    \end{enumerate}

\end{itemize}

\newpage
%----------------------------------------------------------------------------------------

\section{Initial approach}
\label{initialapproach}

In this section, the initial model and its next iterations will be discussed and explained along with the different design options taken during the process.

As mentioned in previous sections this thesis constitutes a Multi-Label classification problem,
so using the related work and state of art \ref{multilabelclassification} as a starting point there are two main architectures:

\subsection{Regular Multi Layer Perceptron (MLP)}
\label{approach MLP}

This method produces a single output for each of the songs fed and it has a neuron in the last layer for every genre $N$ available. Each one of those \(N\) neurons will hold the probability of that genre being present given an example \(x\) using the following expression:
$$ P(\hat{y} = i | x) \forall i \in N$$
This produces a probability vector $\hat{Y}$ that is pruned using a threshold value to obtain the highest probability genres.

The target $Y$ is obtained using the following mapping
$$
\bar{y_i} =
\begin{cases} 
    1 & \text{if } y_j \in S\\
    0 & \text{if } y_j \notin S\\
\end{cases}
\forall j \in N
$$

So given an example with the following genres:

$$ y  = [ Pop, Rock ]$$

And the available genres in the set:
$$ S  = [Jazz, Pop, Rock, Soul]$$
Will produce the output:
$$ \bar{y}  = [0, 1, 1, 0]$$

Then $Y$  is constructed as follows
$$ Y = [y_0, y_1, y_2 ... y_m]^T$$
Where $m$ is the number of examples to feed to the network.

To avoid overfitting it is important to provide a balanced dataset $Y$ otherwise the Network may suffer from overfitting.
To do so some artificial balancing methods such as SMOTE\cite{Chawla2002} \cite{Blagus2013} may be used

In general:
$$ |y^i| \approx \frac{m}{N}   \forall i \in N $$
being:
$$ m > N $$
And:
$$ \sum_{i=1}^{N} |y^i| \approx m $$

\subsection{Multi-label Multi-class Perceptron (MMP)}
\label{approach MMP}

This method differs from the previous one as several classifiers $c_i$ are trained for each of the available genres $N$. 
Each one of the classifiers will provide the statistical probability of a song being of that genre.
This creates an ensemble of classifiers $C$ :
$$ C = \{c_1, c_2, c_3 ... c_N\} $$ 

So given a predict function  $f(x, c)$, an example $x$ and a classifier $c_i$ 

$$ f(x, c_i) =  P(\hat{y_i} = i | x) \forall i \in N $$

Therefore the complete predicted vector $\hat{y}$ is defined as follows:
$$ \hat{y} = [ \hat{y_1}, \hat{y_2}, \hat{y_3} ... \hat{y_N} ] $$

In this case, the target matrix \(Y\) constitutes a column vector obtained using the following mapping:
$$ {y_i} =
\begin{cases}
    1 & \text{if } y_i \in S\\
    0 & \text{if } y_i \notin S\\
\end{cases}
$$
Therefore:
$$ Y = [y_0, y_1, y_2 ... y_m]^T $$
Where $m$ is the number of examples.

Given a binary classifier, the important point here is to have a similar number of positives examples $y^+$ as negative ones $y^-$ so:
$$ |y^+|  \approx \frac{m}{2} $$
$$ |y^-|  \approx \frac{m}{2} $$
$$ |y^+| + |y^-| \approx m $$

For example, for the classifier $c_{Pop}$ and the song :
$$ y_t  = [ Pop, Rock ] $$
Will produce the output:
$$ \bar{y_t}  = 1 $$

On the contrary, the song:
$$ y_k  = [ Jazz, Soul ] $$
Will produce the output:
$$ \bar{y_k}  = 0 $$

\newpage
%----------------------------------------------------------------------------------------

\section{Feature selection}
\label{feature selection}

This section is purely dedicated to explain how the training matrix $X$ is built.
This matrix is used regardless of the network architecture and works in any iteration of the model

The main important aspect to weight in is the number of single features to choose as it will affect the number of neurons in the input layer.

To see all the available features of the provided dataset refer to Appendix \ref{AppendixA}

According to the base material the most promising features to elaborate a genre classifier model are the MFCC \cite{Jensen2006} \cite{Li2011}.

The mentioned features are stacked into a vector $x_i$ and normalized to avoid overweighting the rest of the features 
Then the input matrix is built using:

$$X = [x_1, x_2, x_3, ... x_m]$$

%----------------------------------------------------------------------------------------

\section{Data processing}
\label{data processing}

To avoid having a heavy model on the input and obtain the most prominent features a PCA is applied thus reducing the number of features in the input \cite{pca} \cite{Minka2001}: 
Given $X$ belonging to $\mathbb{R}^m$ the idea is finding a reduction of $ X' \in \mathbb{R}^n$ where $n < m$ that explains a certain percentage of the variance of the original matrix $X$.

When doing predictions and tests it is important to reduce the new matrixes to prune the number of components to $n$. 

\newpage
%----------------------------------------------------------------------------------------

\section{ANN architecture}
The architectures for both approaches are shown below. 
Both models are constructed using a Feed-forward MLP consisting of several layers

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.4\linewidth]{Figures/mlp.png}
        \caption[MLP Architecture]{Regular Multi Layer Perceptron Architecture (see \ref{approach MLP})}
        \label{fig:annmlp}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=.4\linewidth]{Figures/mmlp.png}
        \caption[MMP Architecture]{Multi-Label Multi-Class Perceptron  (see \ref{approach MMP})}
        \label{fig:annmmp}
    \end{subfigure}
    \caption{Artifical Neural Networks architectures}
    \label{fig:architectures}
\end{figure}

\FloatBarrier
The only difference for both architectures is the number of neurons in the output. 
While the MLP provides an output for each of the available genres (this number differs depending on the source dataset) 
the MMP approach uses only 1 similar to a binary classifier and the architecture is repeated for each of the available genres.

To ease the prototyping and model iteration a JSON configuration file is provided to net to interpret and set all the paremeters
This file is explained in detail in \ref{AppendixH}
%----------------------------------------------------------------------------------------

\section{Training}
\label{training phase}
The following graphs show the how the accuracy and loss curves develop along the epochs elapsed during the training.

Given early stopping methods are imposed during the training to avoid overfitting and not accuracy improving most of the models do not consume the maximum total of epochs defined by the user.

Once the training is finished save the trained model to disk and its weights matrixes and the model state with all the hyperparameters.

\subsection{MLP Approach}
\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/training_acc.PNG}
    \decoRule
    \caption[MLP training accuracy]{MLP training accuracy over epoch (On orange: discogs datasource, blue: lastfm datasource and grey: tagtraum datasource)}
    \label{fig:MLP training accuracy}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/training_loss.PNG}
    \decoRule
    \caption[MLP training loss]{MLP training loss over epoch (On orange: discogs datasource, blue: lastfm datasource and grey: tagtraum datasource)}
    \label{fig:MLP training loss}
\end{figure}

This data may be misleading as a great accuracy and loss results are obatined but that is not the case more on \ref{test stage}.
\FloatBarrier
\subsection{MMP Approach}

\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/mmp_acc.pdf}
    \decoRule
    \caption[MMP training accuracy]{MMP training accuracy over epoch for each available genre in the datasource}
    \label{fig:MMP training accuracy}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/mmp_loss.pdf}
    \decoRule
    \caption[MMP training loss]{MLP training loss over epoch for each available genre in the datasource}
    \label{fig:MMP training loss}
\end{figure}

From these graphs we can see the accuracy of a single classifier is about $0.5\%$.

As previously mentioned this data may be misleading as this is the accuracy of a single classifier.

The proper accuracy comes from correctly classifying all the genres from an example.
\newpage
%----------------------------------------------------------------------------------------

\section{Validation}
To avoid overfitting and reaching $100\%$ accuracy in the training set and underperform in the test set a validation set is constructed from the training one and fed to the model too.
After each of the epochs in completed the network evaluates its performance against the validation set , which is not used for training and adjusts the weights accordingly in case of overfitting.

We get that

$$X = X^{Train} \cup X^{Validation}$$
$$X^{Train} \cap X^{Validation} =  \emptyset $$

Between each layer, an additional Dropout layer is included to add random uniform noise and obtain a better performance \cite{Srivastava2014}.

On top of that to validate the overall performance a k-fold cross validation method is used \cite{Rodriguez2010}.
This method divides the entire dataset into $k$ random independent sets and trains the model $k$ times using $k - 1$ sets as training and the remaining one as test
To calculate the accuracy we can get use a simple mean from all the models accuracy

$$acc_{m} = \frac{1}{k}\sum_{i=1}^{k} acc_{m_{i}}$$

\newpage
%----------------------------------------------------------------------------------------

\section{Test}
\label{test stage}

Since the accuracy value is not good enough for classification models a better metric explaining the real accuracy is needed. 
As explained in \cite{Flach2015}, \cite{Davis2006} and suggested by the contest holders the best metrics are Precision, Recall, and F1-score.
\begin{figure}[th]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/cm.png}
    \decoRule
    \caption{Confusion matrix summary}
    \label{fig:Confusion matrix summary}
\end{figure}
These metrics are based on the following concepts:
\begin{itemize}
	\item True Negatives ($TN$) - Samples that were correctly identified as negatives
	\item False Negatives ($FN$) - Samples that were wrongly identified as negatives
	\item True Positives ($TP$) - Samples that were correctly identified as positives
	\item False Positives ($FP$) - Samples that were wrongly identified as positives
\end{itemize}

Depending on the task one may want to maximize or minimize one of the values. 

As a rule of thumb, the higher the $TN$ and $TP$ and the lower the $FN$ and $FP$ the better.

From here we can derive some equations we are interested in:
$$ Precision = \frac{TP}{TP + FP} $$

$$ Recall = \frac{TP}{TP + FN} $$

And the F1 score that combines both metrics:
$$ F_1 = 2*\frac{Precision * Recall}{Precision + Recall} $$

The following tables include a summary of the values obatined for the previously mentioned metrics for each approach and datasource

\begin{table}[h!]
    \centering
    \begin{tabular}{l l l l} 
        \hline
         & discogs & lastfm & tagtraum \\ [0.5ex] 
        \hline
        Precision & 0.0491 & 0.0676 & 0.0792 \\ 
        Recall &  0.2263 &  0.2512 &  0.3017 \\
        F1-score &  0.0806 &  0.1065  & 0.1254 \\
        \hline
    \end{tabular}
    \caption{MLP approach test results}
    \label{table:MLP approach test results}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{l l l l} 
        \hline
         & discogs & lastfm & tagtraum \\ [0.5ex] 
        \hline
        Precision & 0.0863 & 0.1012 & 0.1398 \\ 
        Recall &  0.3249 &  0.2945 &  0.3906 \\
        F1-score &  0.1363 &  0.1506  & 0.2059 \\
        \hline
    \end{tabular}
    \caption{MMP approach test results}
    \label{table:MMP approach test results}
\end{table}

These values present a better reality that the one faced during the training. 

Even though as stated in the state of the art the MMP performances better both approaches provide disappointing results values. 

The MLP approach reached high levels in plain accuracy as the model decided to label most examples as 0 in all its output thus creating a fake effect of accuracy as most samples do not belong to more than 3 genres so there are more 0s than 1s, therefore, increasing accuracy. 

The MMP seems to perform just a bit better, even though having almost random guesses for every genre. With dynamic thresholding
and apriori association rules the results got improved but nonetheless still unacceptable results to deploy any model.

%----------------------------------------------------------------------------------------

\section{Prediction}

When labeling an unknown song once the model is trained and deployed there are two possibilities:

In case the song provided is a raw audio format (.mp3, .wav, .aac) first the deep features need to be obtained using the Essentia software \cite{essentia}. Once done, the new example can be fed on the next step.

Given the provided song already has its features extracted a new example is built using the same procedure as mentioned in \ref{feature selection} generating a new example $x$.

Following the variables are pruned using the PCA as mentioned in \ref{data processing} retaining the biggest $n$ components that were established during the model training phase \ref{training phase}.

Afterward, the model is loaded from disk and the prediction $\hat{y}$ is calculated using the new input $x$. 
This provides an output vector with numerical data, this vector should be transformed into a labeled vector using the inverse function of the one-hot encoding fashion as described in \ref{approach MLP} and \ref{approach MMP}.

This process is briefly outlined in \ref{fig:Predict Stage}

%----------------------------------------------------------------------------------------

