@misc{Silla2011,
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02293v1},
author = {Silla, Carlos N. and Freitas, Alex A.},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-010-0175-9},
eprint = {arXiv:1507.02293v1},
isbn = {1384-5810},
issn = {13845810},
keywords = {DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
pmid = {19477997},
title = {{A survey of hierarchical classification across different application domains}},
year = {2011}
}
@book{Suykens1996,
author = {Suykens, Johan A. K. and Vandewalle, Joos P. L. and {De Moor}, Bart L. R.},
booktitle = {Artificial Neural Networks for Modelling and Control of Non-Linear Systems},
doi = {10.1007/978-1-4757-2493-6},
title = {{Artificial Neural Networks for Modelling and Control of Non-Linear Systems}},
year = {1996}
}
@inproceedings{Mckay2004,
abstract = {This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98{\%} for root genres and 90{\%} for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres.},
author = {Mckay, Cory and Fujinaga, Ichiro},
booktitle = {5th International Conference on Music Information Retrieval – ISMIR 2004},
doi = {S0091-3057(99)00042-8 [pii]},
isbn = {84-88042-44-2},
keywords = {classification,features,genre,hierarchical,music},
pmid = {10418795},
title = {{Automatic Genre Classification Using Large High-Level Musical Feature Sets}},
year = {2004}
}
@article{Werbos1990,
abstract = {Basic backpropagation, which is a simple method now being widely$\backslash$nused in areas like pattern recognition and fault diagnosis, is reviewed.$\backslash$nThe basic equations for backpropagation through time, and applications$\backslash$nto areas like pattern recognition involving dynamic systems, systems$\backslash$nidentification, and control are discussed. Further extensions of this$\backslash$nmethod, to deal with systems other than neural networks, systems$\backslash$ninvolving simultaneous equations, or true recurrent networks, and other$\backslash$npractical issues arising with the method are described. Pseudocode is$\backslash$nprovided to clarify the algorithms. The chain rule for ordered$\backslash$nderivatives-the theorem which underlies backpropagation-is briefly$\backslash$ndiscussed. The focus is on designing a simpler version of$\backslash$nbackpropagation which can be translated into computer code and applied$\backslash$ndirectly by neutral network users},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
issn = {15582256},
journal = {Proceedings of the IEEE},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
year = {1990}
}
@inproceedings{Wang2016,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.251},
isbn = {9781467388504},
issn = {10636919},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
year = {2016}
}
@inproceedings{Rajanna2016,
author = {Rajanna, Arjun Raj and Aryafar, Kamelia and Shokoufandeh, Ali and Ptucha, Raymond},
booktitle = {Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015},
doi = {10.1109/ICMLA.2015.160},
isbn = {9781509002870},
title = {{Deep neural networks: A case study for music genre classification}},
year = {2016}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
year = {2014}
}
@book{EuropeanCommission2009,
author = {{European Commission}},
booktitle = {Luxembourg: Office for Official Publications of the {\ldots}},
doi = {10.2766/88064},
isbn = {9789279097287},
title = {{ECTS users' guide}},
year = {2009}
}
@inproceedings{Jensen2006,
author = {Jensen, Jesper H{\o}jvang and Christensen, Mads Gr{\ae}sb{\o}ll and Murthi, Manohar N. and Jensen, S{\o}ren Holdt},
booktitle = {European Signal Processing Conference},
issn = {22195491},
title = {{Evaluation of MFCC estimation techniques for music similarity}},
year = {2006}
}
@inproceedings{Li2011,
author = {Li, Tom L.H. and Chan, Antoni B.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-17832-0_30},
isbn = {3642178316},
issn = {03029743},
title = {{Genre classification and the invariance of MFCC features to key and tempo}},
year = {2011}
}
@article{InformationCommissionersOffice2018,
abstract = {The Guide to the GDPR explains the provisions of the GDPR to help organisations comply with its requirements. It is for those who have day-to-day responsibility for data protection. This is a living document and we are working to expand it in key areas. It includes links to relevant sections of the GDPR itself, to other ICO guidance and to guidance produced by the EU's Article 29 Working Party. The Working Party includes representatives of the data protection authorities from each EU member state, and the ICO is the UK's representative.},
archivePrefix = {arXiv},
arxivId = {Bird {\&} Bird. (2016). Guide to the General Data Protection Regulation, (April). Retrieved from http://www.twobirds.com/{\~{}}/media/pdfs/gdpr-pdfs/bird--bird--guide-to-the-general-data-protection-regulation.pdf?la=en},
author = {{Information Commissioner's Office}},
doi = {10.1111/j.1751-1097.1994.tb09662.x},
eprint = {/www.twobirds.com/{\~{}}/media/pdfs/gdpr-pdfs/bird--bird--guide-to-the-general-data-protection-regulation.pdf?la=en},
isbn = {1469-896X (Electronic)$\backslash$r0961-8368 (Linking)},
issn = {17511097},
journal = {Guide to the General Data Protection Regulation},
pmid = {15139834},
primaryClass = {Bird {\&} Bird. (2016). Guide to the General Data Protection Regulation, (April). Retrieved from http:},
title = {{Guide to the General Data Protection Regulation (GDPR)}},
year = {2018}
}
@article{Koller1997,
abstract = {The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. One can use existing classifiers by ignoring the hierarchical structure treating the topics as separate classes. Unfortunately in the con text of text categorization we are faced with a large num ber of classes and a huge number of relevant features needed to distinguish between them. Consequently we are restricted to using only very simple classifiers both because of computational cost and the tendency of complex models to overfit. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task in to a set of simpler problems one at each node in the classification tree. As we show each of these smaller problems can be solved accurately by focusing only on a very small set of features those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy so that while the overall relevant feature set may be large each classifier only examines a small subset. The use of reduced feature sets allows us to utilize more complex probabilistic models without encoun tering the computational and robustness difficulties described above.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Koller, Daphne and Sahami, Mehran},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {1558604863},
issn = {13514180},
journal = {International Conference on Machine Learning},
keywords = {author,cs,edu,email address of contact,feature selec-,hierarchical classi cation,information retrieval,probabilistic models,sahami,stanford,tion},
pmid = {15991970},
title = {{Hierarchically Classifying Documents Using Very Few Words}},
year = {1997}
}
@inproceedings{Grodzicki2008a,
abstract = {This paper considers the multilabel classification problem, which$\backslash$nis a generalization of traditional two-class or multi-class classification$\backslash$nproblem. In multilabel classification a set of labels (categories)$\backslash$nis given and each training instance is associated with a subset of$\backslash$nthis label-set. The task is to output the appropriate subset of labels$\backslash$n(generally of unknown size) for a given, unknown testing instance.$\backslash$nSome improvements to the existing neural network multilabel classification$\backslash$nalgorithm, named BP-MLL, are proposed here. The modifications concern$\backslash$nthe form of the global error function used in BP-MLL. The modified$\backslash$nclassification system is tested in the domain of functional genomics,$\backslash$non the yeast genome data set. Experimental results show that proposed$\backslash$nmodifications visibly improve the performance of the neural network$\backslash$nbased multilabel classifier. The results are statistically significant.$\backslash$n漏 2008 Springer-Verlag Berlin Heidelberg.},
author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87700-4_41},
isbn = {3540876995},
issn = {03029743},
keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
title = {{Improved multilabel classification with neural networks}},
year = {2008}
}
@inproceedings{Grodzicki2008,
abstract = {This paper considers the multilabel classification problem, which is a generalization of traditional two-class or multi-class classification problem. In multilabel classification a set of labels (categories) is given and each training instance is associated with a subset of this label-set. The task is to output the appropriate subset of labels (generally of unknown size) for a given, unknown testing instance. Some improvements to the existing neural network multilabel classification algorithm, named BP-MLL, are proposed here. The modifications concern the form of the global error function used in BP-MLL. The modified classification system is tested in the domain of functional genomics, on the yeast genome data set. Experimental results show that proposed modifications visibly improve the performance of the neural network based multilabel classifier. The results are statistically significant. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87700-4_41},
isbn = {3540876995},
issn = {03029743},
keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
title = {{Improved multilabel classification with neural networks}},
year = {2008}
}
@article{Charte2015,
abstract = {Learning from imbalanced data is a problem which arises in many real-world scenarios, so does the need to build classifiers able to predict more than one class label simultaneously (multilabel classification). Dealing with imbalance by means of resampling methods is an approach that has been deeply studied lately, primarily in the context of traditional (non-multilabel) classification. In this paper the process of synthetic instance generation for multilabel datasets (MLDs) is studied and MLSMOTE (Multilabel Synthetic Minority Over-sampling Technique), a new algorithm aimed to produce synthetic instances for imbalanced MLDs, is proposed. An extensive review on how imbalance in the multilabel context has been tackled in the past is provided, along with a thorough experimental study aimed to verify the benefits of the proposed algorithm. Several multilabel classification algorithms and other multilabel oversampling methods are considered, as well as ensemble-based algorithms for imbalanced multilabel classification. The empirical analysis shows that MLSMOTE is able to improve the classification results produced by existent proposals.},
author = {Charte, Francisco and Rivera, Antonio J. and {Del Jesus}, Mar{\'{i}}a J. and Herrera, Francisco},
doi = {10.1016/j.knosys.2015.07.019},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Imbalanced learning,Multilabel classification,Oversampling,Synthetic instance generation},
title = {{MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation}},
year = {2015}
}
@article{Gibaja2014,
author = {Gibaja, Eva and Ventura, Sebastian},
doi = {10.1002/widm.1139},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {may},
title = {{Multilabel Learning: A Review of the State of The Art and Ongoing Research}},
year = {2014}
}
@article{Zhang2006,
abstract = {In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e. Backpropagation for Multi-Label Learning, is proposed. It is derived from the popular Backpropagation algorithm through employing a novel error function capturing the characteristics of multi-label learning, i.e. the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real- world multi-label learning problems, i.e. functional genomics and text categorization, show that the performance of BP-MLL is superior to those of some well-established multi-label learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1683770},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1109/TKDE.2006.162},
eprint = {1683770},
isbn = {8422673517},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Backpropagation,Data mining,Functional genomics,Machine learning,Multilabel learning,Neural networks,Text categorization},
title = {{Multilabel neural networks with applications to functional genomics and text categorization}},
year = {2006}
}
@inproceedings{Flach2015,
abstract = {Precision-Recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance. Perhaps inspired by the many advantages of receiver operating characteristic (ROC) curves and the area under such curves for accuracybased performance assessment, many researchers have taken to report Precision-Recall (PR) curves and associated areas as performance metric. We demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions-e.g., the area under a PR curve takes the arithmetic mean of precision values whereas the F$\beta$ score applies the harmonic mean. We show how to fix this by plotting PR curves in a different coordinate system, and demonstrate that the new Precision-Recall-Gain curves inherit all key advantages of ROC curves. In particular, the area under Precision-Recall-Gain curves conveys an expected F1 score on a harmonic scale, and the convex hull of a Precision-Recall-Gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of $\beta$ values for which the point optimises F$\beta$. We demonstrate experimentally that the area under traditional PR curves can easily favour models with lower expected F1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection.},
author = {Flach, Peter A. and Kull, Meelis},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Precision-Recall-Gain curves: PR analysis done right}},
year = {2015}
}
@article{OrganizacionMundialdelaPropiedadIntelectual-OMPI-2016,
abstract = {Este folleto viene a ser una introducci{\'{o}}n destinada a no especialistas y personas no familiarizadas con la propiedad industrial. En {\'{e}}l se explican en t{\'{e}}rminos llanos los principios en los que se basan los derechos de propiedad industrial. Se describen, por consiguiente, las formas m{\'{a}}s comunes que adopta la propiedad industrial, a saber, las patentes y los modelos de utilidad en relaci{\'{o}}n con las invenciones, los dise{\~{n}}os industriales, las marcas y las indicaciones geogr{\'{a}}ficas y se explican los medios que ofrece el sistema de propiedad industrial a los creadores para proteger sus creaciones. En el folleto no se ofrece orientaci{\'{o}}n jur{\'{i}}dica ni administrativa detallada, por ejemplo, sobre la forma de solicitar protecci{\'{o}}n o de proceder ante una infracci{\'{o}}n de derechos de propiedad industrial, por cuanto esa informaci{\'{o}}n puede solicitarse en las oficinas nacionales de propiedad intelectual. En la contraportada del folleto se ofrece informaci{\'{o}}n sobre sitios Web y publicaciones de utilidad para todo lector que desee profundizar en la materia. En cuanto al derecho de autor, cabe remitirse a una publicaci{\'{o}}n similar de la OMPI, a saber, "Principios b{\'{a}}sicos del derecho de autor y los derechos conexos"},
author = {{Organizaci{\'{o}}n Mundial de la Propiedad Intelectual -OMPI-}},
isbn = {978-92-805-1615-9},
journal = {Organizacion Mundial de la Propiedad Intelectual},
title = {{Principios B{\'{a}}sicos de la Propiedad Industrial}},
year = {2016}
}
@article{Tsoumakas2011,
abstract = {A simple yet effective multilabel learning method, called label powerset (LP), considers each distinct combination of labels that exist in the training set as a different class value of a single-label classification task. The computational efficiency and predictive performance of LP is challenged by application domains with large number of labels and training examples. In these cases, the number of classes may become very large and at the same time many classes are associated with very few training examples. To deal with these problems, this paper proposes breaking the initial set of labels into a number of small random subsets, called labelsets and employing LP to train a corresponding classifier. The labelsets can be either disjoint or overlapping depending on which of two strategies is used to construct them. The proposed method is called {\{}$\backslash$rm RA{\}}k{\{}$\backslash$rm EL{\}} (RAndom k labELsets), where k is a parameter that specifies the size of the subsets. Empirical evidence indicates that {\{}$\backslash$rm RA{\}}k{\{}$\backslash$rm EL{\}} manages to improve substantially over LP, especially in domains with large number of labels and exhibits competitive performance against other high-performing multilabel learning methods.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
doi = {10.1109/TKDE.2010.164},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Categorization,classification,ensembles,labelset,multilabel},
pmid = {6256800},
title = {{Random k-labelsets for multilabel classification}},
year = {2011}
}
@article{Rodriguez2010,
abstract = {In the machine learning field, the performance of a classifier is usually measured in terms of prediction error. In most real-world problems, the error cannot be exactly calculated and it must be estimated. Therefore, it is important to choose an appropriate estimator of the error. This paper analyzes the statistical properties, bias and variance, of the kappa-fold cross-validation classification error estimator (k-cv). Our main contribution is a novel theoretical decomposition of the variance of the kappa-cv considering its sources of variance: sensitivity to changes in the training set and sensitivity to changes in the folds. The paper also compares the bias and variance of the estimator for different values of kappa. The experimental study has been performed in artificial domains because they allow the exact computation of the implied quantities and we can rigorously specify the conditions of experimentation. The experimentation has been performed for two classifiers (naive Bayes and nearest neighbor), different numbers of folds, sample sizes, and training sets coming from assorted probability distributions. We conclude by including some practical recommendation on the use of kappa-fold cross validation.},
author = {Rodr{\'{i}}guez, Juan Diego and P{\'{e}}rez, Aritz and Lozano, Jose Antonio},
doi = {10.1109/TPAMI.2009.187},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {bias and variance,decomposition of the variance,error estimation,k-fold cross validation,prediction error,sources of sensitivity,supervised classification},
title = {{Sensitivity Analysis of k-Fold Cross Validation in Prediction Error Estimation}},
year = {2010}
}
@article{Blagus2013,
author = {Blagus, Rok and Lusa, Lara},
doi = {10.1186/1471-2105-14-106},
issn = {14712105},
journal = {BMC Bioinformatics},
title = {{SMOTE for high-dimensional class-imbalanced data}},
year = {2013}
}
@article{Chawla2002,
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
title = {{SMOTE: Synthetic minority over-sampling technique}},
year = {2002}
}
@article{Bostrom2011,
abstract = {Frankish (Cambridge University Press, 2011): forthcoming The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might differ from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill. Ethics in Machine Learning and Other Domain-Specific AI Algorithms Imagine, in the near future, a bank using a machine learning algorithm to recommend mortgage applications for approval. A rejected applicant brings a lawsuit against the bank, alleging that the algorithm is discriminating racially against mortgage applicants. The bank replies that this is impossible, since the algorithm is deliberately blinded to the race of the applicants. Indeed, that was part of the bank's rationale for implementing the system. Even so, statistics show that the bank's approval rate for black applicants has been steadily dropping. Submitting ten apparently equally qualified genuine applicants (as determined by a separate panel of human judges) shows that the algorithm accepts white applicants and rejects black applicants. What could possibly be happening? Finding an answer may not be easy. If the machine learning algorithm is based on a complicated neural network, or a genetic algorithm produced by directed evolution, then it may prove nearly impossible to understand why, or even how, the algorithm is judging applicants based on their race. On the other hand, a machine learner based on decision trees or Bayesian networks is much more transparent to programmer},
author = {Bostrom, Nick and Yudkowsky, Eliezer},
doi = {10.1017/CBO9781139046855.020},
isbn = {978-0521871426},
issn = {13573039},
journal = {Cambridge University Press},
keywords = {artificial intelligence,ethics},
pmid = {14748188},
title = {{THE ETHICS OF ARTIFICIAL INTELLIGENCE (2011) Nick Bostrom Eliezer Yudkowsky Draft for Cambridge Handbook of Artificial Intelligence, eds}},
year = {2011}
}
@inproceedings{Bogdanov2018,
author = {Bogdanov, Dmitry and Porter, Alastair and Urbano, Juli{\'{a}}n and Schreiber, Hendrik},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
title = {{The MediaEval 2018 AcousticBrainz Genre Task: Content-based music genre recognition from multiple sources}},
year = {2018}
}

@inproceedings{Minka2001,
abstract = {A central issue in principal component analysis (PCA) is choosing the number of principal components to be retained. By interpreting PCA as density estimation, we show how to use Bayesian model selection to estimate the true dimensionality of the data. The resulting estimate is simple to compute yet guaranteed to pick the correct dimensionality, given enough data. The estimate involves an integral over the Steifel manifold of k-frames, which is difficult to compute exactly. But after choosing an appropriate parameterization and applying Laplace's method, an accurate and practical estimator is obtained. In simulations, it is convincingly better than cross-validation and other proposed algorithms, plus it runs much faster.},
author = {Minka, Thomas P.},
booktitle = {Advances in Neural Information Processing Systems},
isbn = {0262122413},
issn = {10495258},
title = {{Automatic choice of dimensionality for PCA}},
year = {2001}
}

@inproceedings{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1143844.1143874},
isbn = {1595933832},
title = {{The relationship between precision-recall and ROC curves}},
year = {2006}
}

@article{Porter2015,
abstract = {We introduce the AcousticBrainz project, an open plat- form for gathering music information. At its core, Acous- ticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Mu- sic Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis re- sults to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to var- ious sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classi- fiers aimed at adding musically relevant semantic informa- tion. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be ac- cessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this commu- nity that will define the actual uses and applications of its data. 1.},
author = {Porter, Alastair and Bogdanov, Dmitry and Kaye, Robert and Tsukanov, Roman and Serra, Xavier and Group, Music Technology and Fabra, Universitat Pompeu and Foundation, Metabrainz},
journal = {16th International Society for Music Information Retrieval Conference (ISMIR 2015)},
title = {{Acousticbrainz: a community platform for gathering music information obtained from audio}},
year = {2015}
}

@inproceedings{Bogdanov2013,
abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
author = {Bogdanov, D. and Wack, N. and G{\'{o}}mez, E. and Gulati, S. and Herrera, P. and Mayor, O. and Roma, G. and Salamon, J. and Zapata, J.R. and Serra, X.},
booktitle = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
title = {{ESSENTIA: an Audio Analysis Library for Music Information Retrieval}},
year = {2013}
}

@inproceedings{Crammer2003,
abstract = {A Family of Additive Online Algorithms for Category Ranking Koby Crammer, Yoram$\backslash$nSinger},
author = {Crammer, Koby and Singer, Yoram},
booktitle = {Journal of Machine Learning Research},
doi = {10.1162/153244303322533188},
issn = {15324435},
title = {{A Family of Additive Online Algorithms for Category Ranking}},
year = {2003}
}

@inproceedings{Mencia2009,
abstract = {The pairwise approach to multilabel classification reduces the problem to learning and aggregating preference predictions among the possible labels. A key problem is the need to query a quadratic number of preferences for making a prediction. To solve this problem, we extend the recently proposed QWeighted algorithm for efficient pairwise multiclass voting to the multilabel setting, and evaluate the adapted algorithm on several real-world datasets. We achieve an average-case reduction of classifier evaluations from n2 to n + dn log n, where n is the total number of possible labels and d is the average number of labels per instance, which is typically quite small in real-world datasets. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Menc{\'{i}}a, Eneldo Loza and Park, Sang Hyeun and F{\"{u}}rnkranz, Johannes},
booktitle = {ESANN 2009 Proceedings, 17th European Symposium on Artificial Neural Networks - Advances in Computational Intelligence and Learning},
isbn = {2930307099},
title = {{Efficient voting prediction for pairwise multilabel classification}},
year = {2009}
}

@inproceedings{LozaMencia2008,
abstract = {In this paper we applied multilabel classification algorithms to the EUR-Lex database of legal documents of the European Union. On this document collection, we studied three different multilabel classification problems, the largest being the categorization into the EUROVOC concept hierarchy with almost 4000 classes. We evaluated three algorithms: (i) the binary relevance approach which independently trains one classifier per label; (ii) the multiclass multilabel perceptron algorithm, which respects dependencies between the base classifiers; and (iii) the multilabel pairwise perceptron algorithm, which trains one classifier for each pair of labels. All algorithms use the simple but very efficient perceptron algorithm as the underlying classifier, which makes them very suitable for large-scale multilabel classification problems. The main challenge we had to face was that the almost 8,000,000 perceptrons that had to be trained in the pairwise setting could no longer be stored in memory. We solve this problem by resorting to the dual representation of the perceptron, which makes the pairwise approach feasible for problems of this size. The results on the EUR-Lex database confirm the good predictive performance of the pairwise approach and demonstrates the feasibility of this approach for large-scale tasks. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {{Loza Menc{\'{i}}a}, Eneldo and F{\"{u}}rnkranz, Johannes},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87481-2_4},
isbn = {3540874801},
issn = {03029743},
title = {{Efficient pairwise multilabel classification for large-scale problems in the legal domain}},
year = {2008}
}

@article{Zhang2009,
author = {Zhang, Min-Ling},
year = {2009},
month = {04},
pages = {61-74},
title = {ML-RBF: RBF neural networks for multi-label learning},
volume = {29},
journal = {Neural Processing Letters},
doi = {10.1007/s11063-009-9095-3}
}

@inproceedings{Cerri2012,
abstract = {In Hierarchical Multi-Label Classification (HMC) problems, each example can be classified into two or more classes simultaneously, differently from standard classification. Moreover, the classes are structured in a hierarchy, in the form of either a tree or a directed acyclic graph. Therefore, an example can be assigned to two or more paths from a hierarchical structure, resulting in a complex classification problem with possibly hundreds or thousands of classes. Several methods have been proposed to deal with such problems, some of them employing a single classifier to deal with all classes simultaneously (global methods), and others employing many classifiers to decompose the original problem into a set of subproblems (local methods). In this work, we propose a novel global method called HMC-GA, which employs a genetic algorithm for solving the HMC problem. In our approach, the genetic algorithm evolves the antecedents of classification rules, in order to optimize the level of coverage of each antecedent. Then, the set of optimized antecedents is selected to build the corresponding consequent of the rules (set of classes to be predicted). Our method is compared to state-of-the-art HMC algorithms, in protein function prediction datasets. The experimental results show that our approach presents competitive predictive accuracy, suggesting that genetic algorithms constitute a promising alternative to deal with hierarchical multi-label classification of biological data. {\textcopyright} 2012 ACM.},
author = {Cerri, Ricardo and Barros, Rodrigo C. and {De Carvalho}, Andre C.P.L.F.},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
doi = {10.1145/2245276.2245325},
isbn = {9781450308571},
keywords = {classification rules,genetic algorithms,hierarchical multi-label classification,protein function prediction},
title = {{A genetic algorithm for hierarchical multi-label classification}},
year = {2012}
}

@article{Vens2008,
abstract = {Hierarchical multi-label classification (HMC) is a variant of classification where instances may belong to multiple classes at the same time and these classes are organized in a hierarchy. This article presents several approaches to the induction of decision trees for HMC, as well as an empirical study of their use in functional genomics. We compare learning a single HMC tree (which makes predictions for all classes together) to two approaches that learn a set of regular classification trees (one for each class). The first approach defines an independent single-label classification task for each class (SC). Obviously, the hierarchy introduces dependencies between the classes. While they are ignored by the first approach, they are exploited by the second approach, named hierarchical single-label classification (HSC). Depending on the application at hand, the hierarchy of classes can be such that each class has at most one parent (tree structure) or such that classes may have multiple parents (DAG structure). The latter case has not been considered before and we show how the HMC and HSC approaches can be modified to support this setting. We compare the three approaches on 24 yeast data sets using as classification schemes MIPS's FunCat (tree structure) and the Gene Ontology (DAG structure). We show that HMC trees outperform HSC and SC trees along three dimensions: predictive accuracy, model size, and induction time. We conclude that HMC trees should definitely be considered in HMC tasks where interpretable models are desired.},
author = {Vens, Celine and Struyf, Jan and Schietgat, Leander and D{\v{z}}eroski, Sa{\v{s}}o and Blockeel, Hendrik},
doi = {10.1007/s10994-008-5077-3},
issn = {08856125},
journal = {Machine Learning},
keywords = {Decision trees,Functional genomics,Hierarchical classification,Multi-label classification,Precision-recall analysis},
title = {{Decision trees for hierarchical multi-label classification}},
year = {2008}
}

@article{koutinimediaeval,
  title={MediaEval 2017 AcousticBrainz Genre Task: Multilayer Perceptron Approach},
  author={Koutini, Khaled and Imenina, Alina and Dorfer, Matthias and Gruber, Alexander Rudolf and Schedl, Markus}
}

@article{Aucouturier2003,
abstract = {Musical genre is probably the most popular music descrip- tor. In the context of large musical databases and Electronic Music Distribution, genre is therefore a crucial metadata for the description of music content. However, genre is intrinsi- cally ill-defined and attempts at defining genre precisely have a strong tendency to end up in circular, ungrounded projec- tions of fantasies. Is genre an intrinsic attribute of music titles, as, say, tempo? Or is genre a extrinsic description of the whole piece? In this article, we discuss the various approaches in representing musical genre, and propose to classify these approaches in three main categories: manual, prescriptive and emergent approaches. We discuss the pros and cons of each approach, and illustrate our study with results of the Cuidado IST project.},
author = {Aucouturier, Jean Julien and Pachet, Fran{\c{c}}ois},
doi = {10.1076/jnmr.32.1.83.16801},
issn = {15497879},
journal = {International Journal of Phytoremediation},
title = {{Representing musical genre: A state of the art}},
year = {2003}
}

@inproceedings{Murauer2018,
abstract = {This paper summarizes our contribution to the CrowdAI music genre classification challenge "Learning to Recognise Musical Genre from Audio on the Web" as part of the WebConference 2018. We utilize different approaches from the field of music analysis to predict the music genre of given mp3 music files, including a convolu-tional neural network for spectrogram classification, deep neural networks and ensemble methods using various numerical audio features. Our best results were obtained by an extreme gradient boosting classifier.},
author = {Murauer, Benjamin and Specht, G{\"{u}}nther},
doi = {10.1145/3184558.3191822},
title = {{Detecting Music Genre Using Extreme Gradient Boosting}},
year = {2018}
}

@inproceedings{Li2003,
abstract = {Content-based music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the Internet. Currently little work has been done on automatic music genre classification, and in addition, the reported classification accuracies are relatively low. This paper proposes a new feature extraction method for music genre classification, DWCHs 1 . DWCHs capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficients. Effectiveness of this new feature and of previously studied features are compared using various machine learning classification algorithms, including Support Vector Machines and Linear Discriminant Analysis. It is demonstrated that the use of DWCHs significantly improves the accuracy of music genre classification.},
author = {Li, Tao and Ogihara, Mitsunori and Li, Qi},
booktitle = {SIGIR Forum (ACM Special Interest Group on Information Retrieval)},
issn = {01635840},
keywords = {Feature extraction,Multi-class classification,Music Genre Classification,Wavelet coefficients histogram},
title = {{A Comparative Study on Content-Based Music Genre Classification}},
year = {2003}
}

@inproceedings{Murauer2017,
abstract = {{\textcopyright} 2017 Author/owner(s). This paper summarizes our contribution (team DBIS) to the AcousticBrainz Genre Task: Content-based music genre recognition from multiple sources as part of MediaEval 2017.We utilize a hierarchical set of multilabel classifiers to predict genres and subgenres and rely on a voting scheme to predict labels across datasets.},
author = {Murauer, Benjamin and Mayerl, Maximilian and Tschuggnall, Michael and Zangerle, Eva and Pichl, Martin and Specht, Gunther},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
title = {{Hierarchical multilabel classification and voting for genre classification}},
year = {2017}
}

@article{Franklin2006,
abstract = {Some researchers in the computational sciences have considered music computation, including music reproduction and generation, as a dynamic system, i.e., a feedback process. The key element is that the state of the musical system depends on a history of past states. Recurrent (neural) networks have been deployed as models for learning musical processes. We first present a tutorial discussion of recurrent networks, covering those that have been used for music learning. Following this, we examine a thread of development of these recurrent networks for music computation that shows how more intricate music has been learned as the state of the art in recurrent networks improves. We present our findings that show that a long short-term memory recurrent network, with new representations that include music knowledge, can learn musical tasks, and can learn to reproduce long songs. Then, given a reharmonization of the chordal structure, it can generate an improvisation.},
author = {Franklin, Judy A.},
doi = {10.1287/ijoc.1050.0131},
issn = {08991499},
journal = {INFORMS Journal on Computing},
keywords = {Computer music,LSTM,Music representation,Recurrent neural networks},
title = {{Recurrent neural networks for music computation}},
year = {2006}
}

@inproceedings{Li2010,
abstract = {Music genre classification has been a challenging yet promising task in the field of music information retrieval (MIR). Due to the highly elusive characteristics of audio musical data, retrieving informative and reliable features from audio signals is crucial to the performance of any music genre classification system. Previous work on audio music genre classification systems mainly concentrated on using timbral features, which limits the performance. To address this problem, we propose a novel approach to extract musical pattern features in audio music using convolutional neural network (CNN), a model widely adopted in image information retrieval tasks. Our experiments show that CNN has strong capacity to capture informative features from the variations of musical patterns with minimal prior knowledge provided.},
author = {Li, Tom L.H. and Chan, Antoni B. and Chun, Andy H.W.},
booktitle = {Proceedings of the International MultiConference of Engineers and Computer Scientists 2010, IMECS 2010},
isbn = {9789881701282},
keywords = {Convolutional neural network,Multimedia data mining,Music feature extractor,Music information retrieval},
title = {{Automatic musical pattern feature extraction using convolutional neural network}},
year = {2010}
}

@article{Gwardys2014,
abstract = {Applications of Convolutional Neural Networks (CNNs) to various problems have been the subject of a number of recent studies ranging from image classification and object detection to scene parsing, segmentation 3D volumetric images and action recognition in videos. CNNs are able to learn input data representation, instead of using fixed engineered features. In this study, the image model trained on CNN were applied to a Music Information Retrieval (MIR), in particular to musical genre recognition. The model was trained on ILSVRC-2012 (more than 1 million natural images) to perform image classification and was reused to perform genre classification using spectrograms images. Harmonic/percussive separation was applied, because it is characteristic for musical genre. At final stage, the evaluation of various strategies of merging Support Vector Machines (SVMs) was performed on well known in MIR community - GTZAN dataset. Even though, the model was trained on natural images, the results achieved in this study were close to the state-of-the-art.},
author = {Gwardys, Grzegorz and Grzywczak, Daniel},
doi = {10.2478/eletel-2014-0042},
issn = {20818491},
journal = {International Journal of Electronics and Telecommunications},
keywords = {convolutional neural networks,deep learning,genre classification,music information retrieval,transfer learning},
title = {{Deep image features in music information retrieval}},
year = {2014}
}


@article{Chen2017,
abstract = {Although Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) have yielded impressive performances in a variety of Music Information Retrieval (MIR) tasks, the complementarity among the CNNs of different architectures and that between CNNs and LSTM are seldom considered. In this paper, multi-channel CNNs with different architectures and LSTM are combined into one unified architecture (Multi-Channel Convolutional LSTM, MCCLSTM) to extract high-level music descriptors. First, three channels of CNNs with different shapes of filter are applied on each spectrogram image chunk to extract the pitch-, tempo-, and bass-relevant descriptors, respectively. Then, the outputs of each CNNs channel are concatenated and then passed through a fully connected layer to obtain the fused descriptor. Finally, LSTM is applied on the fused descriptor sequence of the whole track to extract its long-term structure property to obtain the high-level descriptor. To prove the efficiency of the MCCLSTM model, the obtained high-level music descriptor is applied to the music genre classification and emotion prediction task. Experimental results demonstrate that, when compared with the hand-crafted schemes or conventional deep learning (Multi Layer Perceptrons (MLP), CNNs, and LSTM) based ones, MCCLSTM achieves higher prediction accuracy on three music collections with different kinds of semantic tags.},
author = {Chen, Ning and Wang, Shijun},
journal = {ISMIR, International Society for Music Information Retrieval Conference},
title = {{High-Level Music Descriptor Extraction Algorithm Based on Combination of Multi-Channel CNNs and LSTM}},
year = {2017}
}

@article{Costa2017,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92{\%}, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
author = {Costa, Yandre M.G. and Oliveira, Luiz S. and Silla, Carlos N.},
doi = {10.1016/j.asoc.2016.12.024},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Music genre recognition,Neural network applications,Pattern recognition},
title = {{An evaluation of Convolutional Neural Networks for music classification using spectrograms}},
year = {2017}
}


@inproceedings{schreiber2018mediaeval,
  title={MediaEval 2018 AcousticBrainz Genre Task: A CNN Baseline Relying on Mel-Features.},
  author={Schreiber, Hendrik},
  booktitle={MediaEval},
  year={2018}
}

@inproceedings{oramas2018mediaeval,
  title={MediaEval 2018 AcousticBrainz Genre Task: A baseline combining deep feature embeddings across datasets},
  author={Oramas, Sergio and Bogdanov, Dmitry and Porter, Alastair},
  year={2018},
  organization={CEUR Workshop Proceedings}
}



@misc{genretask,
    title = "{AcousticBrainz Genre Task}",
    howpublished = "\url{https://multimediaeval.github.io/2018-AcousticBrainz-Genre-Task/}"
}

@ARTICLE{pca,
       author = {{Jauregui}, Jeffrey L.},
        title = "{Principal component analysis with linear algebra}",
     keywords = {Mathematics(dimension eigenvalues eigenvectors orthogonality},
}

@misc{cs231n,
    title = {Neural Networks},
    howpublished = "\url{http://cs231n.github.io/neural-networks-1/}"
}


@misc{pythonabout,
    title = {About Python},
    howpublished = "\url{https://www.python.org/doc/essays/blurb/}"
}

@misc{pandas,
  title = {pandas},
  howpublished = "\url{https://pandas.pydata.org/}",
}

@misc{numpy,
    title = {NumPy},
    howpublished = "\url{https://numpy.org/}",
}

@misc{scikit,
    title = {scikit-learn},
    howpublished = "\url{https://scikit-learn.org/}",
}

@misc{matplotlib,
    title = {Matplotlib},
    howpublished = "\url{https://matplotlib.org/}",
}

@misc{seaborn,
    title = {seaborn},
    howpublished = "\url{https://seaborn.pydata.org/}",
}

@misc{mongo,
    title = {MongoDB},
    howpublished = "\url{https://www.mongodb.com/}",
}

@misc{json,
    title = {JSON},
    howpublished = "\url{https://www.json.org/}",
}

@misc{tensorflow,
    title = {TensorFlow},
    howpublished = "\url{https://www.tensorflow.org/}",
}

@misc{keras,
    title = {Keras},
    howpublished = "\url{https://keras.io/}"
}


@misc{essentia,
    title = {Essentia},
    howpublished = "\url{https://essentia.upf.edu/}"
}


@misc{essentiafeatures,
    title = {Essentia Music Descriptors},
    howpublished = "\url{https://essentia.upf.edu/documentation/streaming_extractor_music.html}"
}

@misc{mathann,
    title = {Intro to optimization in deep learning: Gradient Descent},
    howpublished = "\url{https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/}"
}

@misc{tesla,
    title = {Tesla hit with another lawsuit over a fatal Autopilot crash},
    howpublished = "\url{https://www.theverge.com/2019/8/1/20750715/tesla-autopilot-crash-lawsuit-wrongful-death}"
}

@misc{opensource,
    title = {Basics of Open Source},
    howpublished = "\url{https://opensource.org/faq}"
}

@misc{opensourcedef,
    title = {The Open Source Definition},
    howpublished = "\url{https://opensource.org/osd}"
}