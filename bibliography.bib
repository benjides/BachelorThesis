@inproceedings{Grodzicki2008,
abstract = {This paper considers the multilabel classification problem, which is a generalization of traditional two-class or multi-class classification problem. In multilabel classification a set of labels (categories) is given and each training instance is associated with a subset of this label-set. The task is to output the appropriate subset of labels (generally of unknown size) for a given, unknown testing instance. Some improvements to the existing neural network multilabel classification algorithm, named BP-MLL, are proposed here. The modifications concern the form of the global error function used in BP-MLL. The modified classification system is tested in the domain of functional genomics, on the yeast genome data set. Experimental results show that proposed modifications visibly improve the performance of the neural network based multilabel classifier. The results are statistically significant. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87700-4_41},
isbn = {3540876995},
issn = {03029743},
keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
title = {{Improved multilabel classification with neural networks}},
year = {2008}
}
@article{Gibaja2014,
author = {Gibaja, Eva and Ventura, Sebastian},
doi = {10.1002/widm.1139},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {may},
title = {{Multilabel Learning: A Review of the State of The Art and Ongoing Research}},
year = {2014}
}
@inproceedings{Wang2016,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
author = {Wang, Jiang and Yang, Yi and Mao, Junhua and Huang, Zhiheng and Huang, Chang and Xu, Wei},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.251},
isbn = {9781467388504},
issn = {10636919},
title = {{CNN-RNN: A Unified Framework for Multi-label Image Classification}},
year = {2016}
}
@article{Charte2015,
abstract = {Learning from imbalanced data is a problem which arises in many real-world scenarios, so does the need to build classifiers able to predict more than one class label simultaneously (multilabel classification). Dealing with imbalance by means of resampling methods is an approach that has been deeply studied lately, primarily in the context of traditional (non-multilabel) classification. In this paper the process of synthetic instance generation for multilabel datasets (MLDs) is studied and MLSMOTE (Multilabel Synthetic Minority Over-sampling Technique), a new algorithm aimed to produce synthetic instances for imbalanced MLDs, is proposed. An extensive review on how imbalance in the multilabel context has been tackled in the past is provided, along with a thorough experimental study aimed to verify the benefits of the proposed algorithm. Several multilabel classification algorithms and other multilabel oversampling methods are considered, as well as ensemble-based algorithms for imbalanced multilabel classification. The empirical analysis shows that MLSMOTE is able to improve the classification results produced by existent proposals.},
author = {Charte, Francisco and Rivera, Antonio J. and {Del Jesus}, Mar{\'{i}}a J. and Herrera, Francisco},
doi = {10.1016/j.knosys.2015.07.019},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Imbalanced learning,Multilabel classification,Oversampling,Synthetic instance generation},
title = {{MLSMOTE: Approaching imbalanced multilabel learning through synthetic instance generation}},
year = {2015}
}
@article{Tsoumakas2011,
abstract = {A simple yet effective multilabel learning method, called label powerset (LP), considers each distinct combination of labels that exist in the training set as a different class value of a single-label classification task. The computational efficiency and predictive performance of LP is challenged by application domains with large number of labels and training examples. In these cases, the number of classes may become very large and at the same time many classes are associated with very few training examples. To deal with these problems, this paper proposes breaking the initial set of labels into a number of small random subsets, called labelsets and employing LP to train a corresponding classifier. The labelsets can be either disjoint or overlapping depending on which of two strategies is used to construct them. The proposed method is called {\{}$\backslash$rm RA{\}}k{\{}$\backslash$rm EL{\}} (RAndom k labELsets), where k is a parameter that specifies the size of the subsets. Empirical evidence indicates that {\{}$\backslash$rm RA{\}}k{\{}$\backslash$rm EL{\}} manages to improve substantially over LP, especially in domains with large number of labels and exhibits competitive performance against other high-performing multilabel learning methods.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
doi = {10.1109/TKDE.2010.164},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Categorization,classification,ensembles,labelset,multilabel},
pmid = {6256800},
title = {{Random k-labelsets for multilabel classification}},
year = {2011}
}
@inproceedings{Grodzicki2008a,
abstract = {This paper considers the multilabel classification problem, which$\backslash$nis a generalization of traditional two-class or multi-class classification$\backslash$nproblem. In multilabel classification a set of labels (categories)$\backslash$nis given and each training instance is associated with a subset of$\backslash$nthis label-set. The task is to output the appropriate subset of labels$\backslash$n(generally of unknown size) for a given, unknown testing instance.$\backslash$nSome improvements to the existing neural network multilabel classification$\backslash$nalgorithm, named BP-MLL, are proposed here. The modifications concern$\backslash$nthe form of the global error function used in BP-MLL. The modified$\backslash$nclassification system is tested in the domain of functional genomics,$\backslash$non the yeast genome data set. Experimental results show that proposed$\backslash$nmodifications visibly improve the performance of the neural network$\backslash$nbased multilabel classifier. The results are statistically significant.$\backslash$nÊºè 2008 Springer-Verlag Berlin Heidelberg.},
author = {Grodzicki, Rafa{\l} and Ma{\'{n}}dziuk, Jacek and Wang, Lipo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87700-4_41},
isbn = {3540876995},
issn = {03029743},
keywords = {Backpropagation,Bioinformatics,Functional genomics,Learning system,Multilabel,Neural network},
title = {{Improved multilabel classification with neural networks}},
year = {2008}
}
@article{Zhang2006,
abstract = {In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, this problem is addressed in the way that a neural network algorithm named BP-MLL, i.e. Backpropagation for Multi-Label Learning, is proposed. It is derived from the popular Backpropagation algorithm through employing a novel error function capturing the characteristics of multi-label learning, i.e. the labels belonging to an instance should be ranked higher than those not belonging to that instance. Applications to two real- world multi-label learning problems, i.e. functional genomics and text categorization, show that the performance of BP-MLL is superior to those of some well-established multi-label learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1683770},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1109/TKDE.2006.162},
eprint = {1683770},
isbn = {8422673517},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Backpropagation,Data mining,Functional genomics,Machine learning,Multilabel learning,Neural networks,Text categorization},
title = {{Multilabel neural networks with applications to functional genomics and text categorization}},
year = {2006}
}
@misc{Silla2011,
abstract = {In this survey we discuss the task of hierarchical classification. The literature about this field is scattered across very different application domains and for that reason research in one domain is often done unaware of methods developed in other domains. We define what is the task of hierarchical classification and discuss why some related tasks should not be considered hierarchical classification. We also present a new perspective about some existing hierarchical classification approaches, and based on that perspective we propose a new unifying framework to classify the existing approaches. We also present a review of empirical comparisons of the existing methods reported in the literature as well as a conceptual comparison of those methods at a high level of abstraction, discussing their advantages and disadvantages.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02293v1},
author = {Silla, Carlos N. and Freitas, Alex A.},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-010-0175-9},
eprint = {arXiv:1507.02293v1},
isbn = {1384-5810},
issn = {13845810},
keywords = {DAG-structured class hierarchies,Hierarchical classification,Tree-structured class hierarchies},
pmid = {19477997},
title = {{A survey of hierarchical classification across different application domains}},
year = {2011}
}
@article{Koller1997,
abstract = {The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. One can use existing classifiers by ignoring the hierarchical structure treating the topics as separate classes. Unfortunately in the con text of text categorization we are faced with a large num ber of classes and a huge number of relevant features needed to distinguish between them. Consequently we are restricted to using only very simple classifiers both because of computational cost and the tendency of complex models to overfit. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task in to a set of simpler problems one at each node in the classification tree. As we show each of these smaller problems can be solved accurately by focusing only on a very small set of features those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy so that while the overall relevant feature set may be large each classifier only examines a small subset. The use of reduced feature sets allows us to utilize more complex probabilistic models without encoun tering the computational and robustness difficulties described above.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Koller, Daphne and Sahami, Mehran},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {1558604863},
issn = {13514180},
journal = {International Conference on Machine Learning},
keywords = {author,cs,edu,email address of contact,feature selec-,hierarchical classi cation,information retrieval,probabilistic models,sahami,stanford,tion},
pmid = {15991970},
title = {{Hierarchically Classifying Documents Using Very Few Words}},
year = {1997}
}
@inproceedings{Mckay2004,
abstract = {This paper presents a system that extracts 109 musical features from symbolic recordings (MIDI, in this case) and uses them to classify the recordings by genre. The features used here are based on instrumentation, texture, rhythm, dynamics, pitch statistics, melody and chords. The classification is performed hierarchically using different sets of features at different levels of the hierarchy. Which features are used at each level, and their relative weightings, are determined using genetic algorithms. Classification is performed using a novel ensemble of feedforward neural networks and k-nearest neighbour classifiers. Arguments are presented emphasizing the importance of using high-level musical features, something that has been largely neglected in automatic classification systems to date in favour of low-level features. The effect on classification performance of varying the number of candidate features is examined in order to empirically demonstrate the importance of using a large variety of musically meaningful features. Two differently sized hierarchies are used in order to test the performance of the system under different conditions. Very encouraging classification success rates of 98{\%} for root genres and 90{\%} for leaf genres are obtained for a hierarchical taxonomy consisting of 9 leaf genres.},
author = {Mckay, Cory and Fujinaga, Ichiro},
booktitle = {5th International Conference on Music Information Retrieval ‚Äì ISMIR 2004},
doi = {S0091-3057(99)00042-8 [pii]},
isbn = {84-88042-44-2},
keywords = {classification,features,genre,hierarchical,music},
pmid = {10418795},
title = {{Automatic Genre Classification Using Large High-Level Musical Feature Sets}},
year = {2004}
}
@inproceedings{Rajanna2016,
author = {Rajanna, Arjun Raj and Aryafar, Kamelia and Shokoufandeh, Ali and Ptucha, Raymond},
booktitle = {Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015},
doi = {10.1109/ICMLA.2015.160},
isbn = {9781509002870},
title = {{Deep neural networks: A case study for music genre classification}},
year = {2016}
}
@book{Suykens1996,
author = {Suykens, Johan A. K. and Vandewalle, Joos P. L. and {De Moor}, Bart L. R.},
booktitle = {Artificial Neural Networks for Modelling and Control of Non-Linear Systems},
doi = {10.1007/978-1-4757-2493-6},
title = {{Artificial Neural Networks for Modelling and Control of Non-Linear Systems}},
year = {1996}
}
@inproceedings{Li2011,
author = {Li, Tom L.H. and Chan, Antoni B.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-17832-0_30},
isbn = {3642178316},
issn = {03029743},
title = {{Genre classification and the invariance of MFCC features to key and tempo}},
year = {2011}
}
@inproceedings{Bogdanov2018,
author = {Bogdanov, Dmitry and Porter, Alastair and Urbano, Juli{\'{a}}n and Schreiber, Hendrik},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
title = {{The MediaEval 2018 AcousticBrainz Genre Task: Content-based music genre recognition from multiple sources}},
year = {2018}
}
@inproceedings{Jensen2006,
author = {Jensen, Jesper H{\o}jvang and Christensen, Mads Gr{\ae}sb{\o}ll and Murthi, Manohar N. and Jensen, S{\o}ren Holdt},
booktitle = {European Signal Processing Conference},
issn = {22195491},
title = {{Evaluation of MFCC estimation techniques for music similarity}},
year = {2006}
}
@article{Chawla2002,
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
title = {{SMOTE: Synthetic minority over-sampling technique}},
year = {2002}
}
@article{Blagus2013,
author = {Blagus, Rok and Lusa, Lara},
doi = {10.1186/1471-2105-14-106},
issn = {14712105},
journal = {BMC Bioinformatics},
title = {{SMOTE for high-dimensional class-imbalanced data}},
year = {2013}
}
@book{EuropeanCommission2009,
author = {{European Commission}},
booktitle = {Luxembourg: Office for Official Publications of the {\ldots}},
doi = {10.2766/88064},
isbn = {9789279097287},
title = {{ECTS users' guide}},
year = {2009}
}

@misc{cs231n,
    title = {Neural Networks},
    howpublished = "\url{http://cs231n.github.io/neural-networks-1/}"
}


@misc{pythonabout,
    title = {About Python},
    howpublished = "\url{https://www.python.org/doc/essays/blurb/}"
}

@misc{pandas,
  title = {pandas},
  howpublished = "\url{https://pandas.pydata.org/}",
}

@misc{numpy,
    title = {NumpPy},
    howpublished = "\url{https://numpy.org/}",
}

@misc{scikit,
    title = {scikit-learn},
    howpublished = "\url{https://scikit-learn.org/}",
}

@misc{matplotlib,
    title = {Matplotlib},
    howpublished = "\url{https://matplotlib.org/}",
}

@misc{seaborn,
    title = {seaborn},
    howpublished = "\url{https://seaborn.pydata.org/}",
}

@misc{mongo,
    title = {MongoDB},
    howpublished = "\url{https://www.mongodb.com/}",
}

@misc{json,
    title = {JSON},
    howpublished = "\url{https://www.json.org/}",
}

@misc{tensorflow,
    title = {TensorFlow},
    howpublished = "\url{https://www.tensorflow.org/}",
}

@misc{keras,
    title = {Keras},
    howpublished = "\url{https://keras.io/}"
}


@misc{essentia,
    title = {Essentia},
    howpublished = "\url{https://essentia.upf.edu/}"
}
